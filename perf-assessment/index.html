

<!doctype html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>COCO: Performance Assessment &mdash; COCO: Performance Assessment</title>
    
    <link rel="stylesheet" href="_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.9',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/bizstyle.js"></script>
    <link rel="top" title="COCO: Performance Assessment" href="#" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="#">COCO: Performance Assessment</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">COCO: Performance Assessment</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a><ul>
<li><a class="reference internal" href="#terminology-and-definitions">Terminology and Definitions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#on-performance-measures">On Performance Measures</a><ul>
<li><a class="reference internal" href="#quality-indicators">Quality Indicators</a></li>
<li><a class="reference internal" href="#fixed-budget-versus-fixed-target-approach">Fixed-Budget versus Fixed-Target Approach</a></li>
<li><a class="reference internal" href="#missing-values">Missing Values</a></li>
<li><a class="reference internal" href="#target-value-setting">Target Value Setting</a></li>
<li><a class="reference internal" href="#runlength-based-target-values">Runlength-based Target Values</a></li>
</ul>
</li>
<li><a class="reference internal" href="#runtime-computation">Runtime Computation</a><ul>
<li><a class="reference internal" href="#runs-on-different-instances">Runs on Different Instances</a></li>
<li><a class="reference internal" href="#simulated-restarts-and-runtimes">Simulated Restarts and Runtimes</a><ul>
<li><a class="reference internal" href="#bootstrapping-runtimes">Bootstrapping Runtimes</a></li>
<li><a class="reference internal" href="#rationales-and-limitations">Rationales and Limitations</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#averaging-runtime">Averaging Runtime</a><ul>
<li><a class="reference internal" href="#motivation">Motivation</a></li>
<li><a class="reference internal" href="#rationale-and-limitations">Rationale and Limitations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#empirical-distribution-functions">Empirical Distribution Functions</a><ul>
<li><a class="reference internal" href="#rationale-interpretation-and-limitations">Rationale, Interpretation and Limitations</a></li>
<li><a class="reference internal" href="#relation-to-previous-work">Relation to Previous Work</a></li>
<li><a class="reference internal" href="#examples">Examples</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="coco-performance-assessment">
<h1>COCO: Performance Assessment<a class="headerlink" href="#coco-performance-assessment" title="Permalink to this headline">¶</a></h1>
See: <I>ArXiv e-prints</I>,
<A HREF="http://arxiv.org/abs/1605.03560">arXiv:1605.03560</A>, 2016.<p>We present an any-time performance assessment for benchmarking numerical
optimization algorithms in a black-box scenario, applied within the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> benchmarking platform.
The performance assessment is based on <em>runtimes</em> measured in number of objective function evaluations to reach one or several quality indicator target values.
We argue that runtime is the only available measure with a generic, meaningful, and quantitative interpretation.
We discuss the choice of the target values, runlength-based targets, and the aggregation of results by using simulated restarts, averages, and empirical distribution functions.</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>We present ideas and concepts for performance assessment when benchmarking numerical optimization algorithms in a black-box scenario.
Going beyond a simple ranking of algorithms, we aim
to provide a <em>quantitative</em> and <em>meaningful</em> performance assessment, which
allows for conclusions like <em>algorithm A is seven times faster than algorithm
B</em> in solving a given problem or in solving problems with certain
characteristics.
For this end, we record algorithm <em>runtimes, measured in
number of function evaluations</em> to reach predefined target values during the
algorithm run.</p>
<p>Runtimes represent the cost of optimization. Apart from a short, exploratory
experiment <a class="footnote-reference" href="#id7" id="id4">[1]</a>, we do not measure the algorithm cost in CPU or wall-clock time.
See for example <a class="reference internal" href="#hoo1995" id="id5">[HOO1995]</a> for a discussion on shortcomings and
unfortunate consequences of benchmarking based on CPU time.</p>
<p>In the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> platform <a class="reference internal" href="#han2016co" id="id6">[HAN2016co]</a>, we display average runtimes (<img class="math" src="_images/math/27971c0d7cdc5cc52c6b37398061cd67d1106a1a.png" alt="\mathrm{aRT}" style="vertical-align: 0px"/>, see Section <a class="reference internal" href="#averaging-runtime">Averaging Runtime</a>)
and the empirical distribution function of runtimes (ECDF, see Section <a class="reference internal" href="#empirical-distribution-functions">Empirical Distribution Functions</a>).
When displaying runtime distributions, we consider the aggregation over
target values and over subclasses of problems, or all problems.</p>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[1]</a></td><td>The <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> platform provides a CPU timing experiment to get a rough estimate of the time complexity of the algorithm <a class="reference internal" href="#han2016ex" id="id8">[HAN2016ex]</a>.</td></tr>
</tbody>
</table>
<div class="section" id="terminology-and-definitions">
<h3>Terminology and Definitions<a class="headerlink" href="#terminology-and-definitions" title="Permalink to this headline">¶</a></h3>
<p>In the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework in general, a <strong>problem</strong>, or problem instance triplet, <img class="math" src="_images/math/8aaa28094337e3019cc83e6c9ef7193a6412258c.png" alt="p^3" style="vertical-align: -4px"/>, is defined by the search space dimension <img class="math" src="_images/math/7bc0d11b5a52732a23025dcc422625c0a5dde474.png" alt="n" style="vertical-align: 0px"/>, the objective function <img class="math" src="_images/math/dd675eae8a6fc90e0e73930e5e09f1c885df11e1.png" alt="f" style="vertical-align: -4px"/>, to be minimized, and its instance parameters <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/> for instance <img class="math" src="_images/math/8e66149081fdbba5cad4420e5977456c05f9f8d3.png" alt="i" style="vertical-align: 0px"/>.
More concisely, we consider a set of parametrized benchmark functions
<img class="math" src="_images/math/5ff7e8176ed6da11645dff8ed875645fd08ce2fe.png" alt="f_\theta: \mathbb{R}^n \to \mathbb{R}^m, \theta \in \Theta" style="vertical-align: -4px"/> and the corresponding problems <img class="math" src="_images/math/ab269c3f1c657b7a51c29aa7d5d998d048205e79.png" alt="p^3 = p(n, f_\theta, \theta_i)" style="vertical-align: -4px"/>.
Different instances vary by having different shifted optima, can use different rotations that are applied to the variables, have different optimal <img class="math" src="_images/math/dd675eae8a6fc90e0e73930e5e09f1c885df11e1.png" alt="f" style="vertical-align: -4px"/>-values, etc. <a class="reference internal" href="#han2009fun" id="id9">[HAN2009fun]</a>.
The instance notion is introduced to generate repetition while avoiding possible exploitation of artificial function properties (like location of the optimum in zero).
The separation of dimension and instance parameters in the notation serves as a hint to indicate that we never aggregate over dimension and always aggregate over all <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/>-values.</p>
<p>In the performance assessment setting, we associate to a problem instance
<img class="math" src="_images/math/8aaa28094337e3019cc83e6c9ef7193a6412258c.png" alt="p^3" style="vertical-align: -4px"/> a quality indicator mapping and a target value, such that a problem becomes a
quintuple <img class="math" src="_images/math/beae7dab2278797db0c5b826c63e5bc2cbca9cbc.png" alt="p^5" style="vertical-align: -4px"/>.
Usually, the quality indicator remains the same for all problems, while we have
subsets of problems which only differ in their target value.</p>
<blockquote>
<div></div></blockquote>
</div>
</div>
<div class="section" id="on-performance-measures">
<h2>On Performance Measures<a class="headerlink" href="#on-performance-measures" title="Permalink to this headline">¶</a></h2>
<p>Evaluating performance is necessarily based on performance <em>measures</em>, the
definition of which plays a crucial role for the evaluation.
Here, we introduce a list of requirements a performance measure should satisfy in general, as well as in the context of black-box optimization specifically.
In general, a performance measure should be</p>
<ul class="simple">
<li>quantitative, as opposed to a simple <em>ranking</em> of entrants (e.g., algorithms).
Ideally, the measure should be defined on a ratio scale (as opposed to an
interval or ordinal scale) <a class="reference internal" href="#ste1946" id="id10">[STE1946]</a>, which allows to state that &#8220;entrant A
is <img class="math" src="_images/math/08c8ce72dec4fe55a32d5a0c9cfb153e20cc7329.png" alt="x" style="vertical-align: 0px"/> <em>times better</em> than entrant B&#8221;. <a class="footnote-reference" href="#id13" id="id11">[2]</a></li>
<li>assuming a wide variation of values such that, for example, typical values do
not only range between 0.98 and 1.0, <a class="footnote-reference" href="#id15" id="id12">[3]</a></li>
<li>well interpretable, in particular by having meaning and semantics attached to
the measured numbers,</li>
<li>relevant and meaningful with respect to the &#8220;real world&#8221;,</li>
<li>as simple and as comprehensible as possible.</li>
</ul>
<p>In the context of black-box optimization, the <strong>runtime</strong> to reach a target value, measured in number of function evaluations, satisfies all requirements.
Runtime is well-interpretable and meaningful with respect to the
real-world as it represents time needed to solve a problem.
Measuring number of function evaluations avoids the shortcomings of CPU
measurements that depend on parameters like the programming language, coding
style, machine used to run the experiment, etc., that are difficult or
impractical to control.
If however algorithm internal computations dominate wall-clock time in a
practical application, comparative runtime results <em>in number of function
evaluations</em> can usually be adapted <em>a posteri</em> to reflect the practical
scenario.
This hold also true for a speed up from parallelization.</p>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id11">[2]</a></td><td>A variable which lives on a ratio scale has a meaningful zero,
allows for division, and can be taken to the logarithm in a meaningful way.
See for example <a class="reference external" href="https://en.wikipedia.org/wiki/Level_of_measurement?oldid=478392481">Level of measurement on Wikipedia</a>.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id12">[3]</a></td><td>A transformation like <img class="math" src="_images/math/84a37d506dd3e5e58d9b42dcba911e9e9516700f.png" alt="x\mapsto\log(1-x)" style="vertical-align: -4px"/> could alleviate the problem
in this case, given it actually zooms in on relevant values.</td></tr>
</tbody>
</table>
<div class="section" id="quality-indicators">
<span id="sec-verthori"></span><h3>Quality Indicators<a class="headerlink" href="#quality-indicators" title="Permalink to this headline">¶</a></h3>
<p>At each evaluation count (time step) <img class="math" src="_images/math/56df6e67a4d69032d8ef78dcb7c348e3ffbc6abd.png" alt="t" style="vertical-align: 0px"/> of an algorithm which optimizes a problem instance <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/> of the function <img class="math" src="_images/math/ca2de076682c37daa8d0fb48ee82481772f5bb12.png" alt="f_{\theta}" style="vertical-align: -4px"/> in dimension <img class="math" src="_images/math/7bc0d11b5a52732a23025dcc422625c0a5dde474.png" alt="n" style="vertical-align: 0px"/>, we apply a quality indicator mapping.
A quality indicator <img class="math" src="_images/math/592e9f34062623dee6b227c81478031be50af639.png" alt="I" style="vertical-align: 0px"/> maps the set of all solutions evaluated
so far (or recommended <a class="reference internal" href="#han2016ex" id="id16">[HAN2016ex]</a>) to a problem-dependent real value.
Then, a runtime measurement can be obtained from each of a (large) set of
problem instances <img class="math" src="_images/math/96c8d4adf3587655982a8b42bc66530e662734ce.png" alt="p^5 = p(n, f_\theta, \theta_i, I, I^\mathrm{target,
\theta_i}_{f})" style="vertical-align: -7px"/>.
The runtime on this problem instance is defined as the evaluation count
when the quality indicator value drops below the target for the first time, otherwise runtime remains undefined.</p>
<p>In the single-objective noiseless case, the quality indicator outputs
the best so far observed (i.e. minimal and feasible) function value.</p>
<p>In the single-objective noisy case, the quality indicator returns the 1%-tile of
the function values of the last <img class="math" src="_images/math/fb6f904fd186e5eb6fbf949087b738ac7e5c810f.png" alt="\lceil\ln(t + 3)^2 / 2\rceil" style="vertical-align: -5px"/> evaluated
(or recommended) solutions. <a class="footnote-reference" href="#id19" id="id17">[4]</a></p>
<p>In the multi-objective case, the quality indicator is based on a negative
hypervolume indicator of the set of evaluated solutions (more specifically, the the non-dominated archive)
<a class="reference internal" href="#bro2016" id="id18">[BRO2016]</a>, while other well- or lesser-known multi-objective quality indicators
are possible.</p>
<table class="docutils footnote" frame="void" id="id19" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id17">[4]</a></td><td>This feature will only be available in the new implementation of the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="fixed-budget-versus-fixed-target-approach">
<h3>Fixed-Budget versus Fixed-Target Approach<a class="headerlink" href="#fixed-budget-versus-fixed-target-approach" title="Permalink to this headline">¶</a></h3>
<p>Starting from the most basic convergence graphs which plot the evolution of a
quality indicator, to be minimized, against the number of function evaluations,
there are essentially only two ways to measure the performance.</p>
<dl class="docutils">
<dt>fixed-budget approach:</dt>
<dd>We fix a maximal budget of function evaluations,
and measure the reached quality indicator value. A fixed search
budget can be pictured as drawing a <em>vertical</em> line in the figure
(blue line in Figure <a class="reference internal" href="#fig-horizontalvsvertical"><span>Fixed-Budget versus Fixed-Target</span></a>).</dd>
<dt>fixed-target approach:</dt>
<dd>We fix a target quality value and measure the number of function
evaluations, the <em>runtime</em>, to reach this target. A fixed target can be
pictured as drawing a <em>horizontal</em> line in the figure (red line in Figure
<a class="reference internal" href="#fig-horizontalvsvertical"><span>Fixed-Budget versus Fixed-Target</span></a>).</dd>
</dl>
<div class="figure align-center" id="id64">
<span id="fig-horizontalvsvertical"></span><a class="reference internal image-reference" href="_images/fixed-budget-vs-fixed-target.svg"><img src="_images/fixed-budget-vs-fixed-target.svg" width="70%" /></a>
<p class="caption"><span class="caption-text"><strong>Fixed-Budget versus Fixed-Target</strong></span></p>
<div class="legend">
Illustration of fixed-budget view (vertical cuts) and fixed-target view
(horizontal cuts). Black lines depict the best quality indicator value
plotted versus number of function evaluations.</div>
</div>
<p>For the performance assessment of algorithms, the fixed-target approach is superior
to the fixed-budget approach since it gives <em>quantitative and interpretable</em>
results.</p>
<ul class="simple">
<li>The fixed-budget approach (vertical cut) does not give <em>quantitatively
interpretable</em>  data:
the observation that Algorithm A reaches a quality indicator value that is, say, two
times smaller than the one reached by Algorithm B has in general no
interpretable meaning, mainly because there is no <em>a priori</em> way to determine
<em>how much</em> more difficult it is to reach an indicator value that is two times
smaller.
This usually depends on the function, the definition of the
quality indicator and even the specific indicator values compared.</li>
<li>The fixed-target approach (horizontal cut)
<em>measures the time</em> to
reach a target quality value. The measurement allows conclusions of the
type: Algorithm A is two (or ten, or a hundred) times faster than Algorithm B
in solving this problem.
The choice of the target value determines the difficulty and
often the characteristic of the problem to be solved.</li>
</ul>
<p>Furthermore, for algorithms that are invariant under certain transformations
of the function value (for example under order-preserving transformations, as
comparison-based algorithms like DE, ES, PSO <a class="reference internal" href="#aug2009" id="id20">[AUG2009]</a>), fixed-target measures are
invariant under these transformations if the target values are transformed accordingly. That is, only the horizontal line needs to be moved. Fixed-budget measures require the transformation of all resulting measurements individually.</p>
</div>
<div class="section" id="missing-values">
<h3>Missing Values<a class="headerlink" href="#missing-values" title="Permalink to this headline">¶</a></h3>
<p>Investigating the Figure <a class="reference internal" href="#fig-horizontalvsvertical"><span>Fixed-Budget versus Fixed-Target</span></a> more carefully, we find that not all graphs intersect with either the vertical or the horizontal line.
On the one hand, if the fixed budget is too large, the algorithm might solve the function before the budget is exceeded. <a class="footnote-reference" href="#id24" id="id21">[5]</a>
The algorithm performs better than the measurement is able to reflect, which can lead to a serious misinterpretations.
The remedy is to define a <em>final</em> target value and measure the runtime if the final target is hit. <a class="footnote-reference" href="#id25" id="id22">[6]</a></p>
<p>On the other hand, if the fixed target is too difficult, the algorithm may never hit the target under the given experimental conditions. <a class="footnote-reference" href="#id26" id="id23">[7]</a>
The algorithm performs worse than the experiment is able to reflect, while we still get a lower bound for this missing runtime instance.
A possible remedy is to run the algorithm longer.
Another possible remedy is to use the final quality indicator value as measurement.
This measurement however should only be interpreted as ranking result, defeating the original objective.
A third (impartial) remedy is to record the overall number of function evaluations of this run and use simulated restarts, see below.</p>
<table class="docutils footnote" frame="void" id="id24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id21">[5]</a></td><td>Even in continuous domain, from the view point of benchmarking,
or application in the real world, or numerical precision, the set of
solutions (or of solution sets) that indisputably solve the problem has a
volume larger than zero.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id25" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id22">[6]</a></td><td>This is also advisable because declaring an algorithm better
when it reaches, say, <img class="math" src="_images/math/18f1e70fa25dddbc86ece36d69e7adbff5209009.png" alt="\mathsf{const} + 10^{-30}" style="vertical-align: -2px"/> instead of
<img class="math" src="_images/math/c4b042bc86f7d28283da8daf7f95f0ec69b88e80.png" alt="\mathsf{const} + 10^{-10}" style="vertical-align: -2px"/>, is more often than not unjustified.
The former result may only indicate the lack of practical
termination conditions.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id23">[7]</a></td><td>However, under mildly randomized conditions, for example with a randomized initial solution, the restarted algorithm reaches any attainable target with probability one. The time needed can of course well be beyond any reasonable practical limitations.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="target-value-setting">
<h3>Target Value Setting<a class="headerlink" href="#target-value-setting" title="Permalink to this headline">¶</a></h3>
<p>First, we define for each problem instance <img class="math" src="_images/math/05c681f406a671e0a32b82c58e0f1c8603eec507.png" alt="p^3 = (n, f_\theta, \theta_i)" style="vertical-align: -4px"/>
a <em>reference</em> quality indicator value, <img class="math" src="_images/math/6ea44c867ae4212225cf52318cae8ef40a67b25b.png" alt="I^{\rm ref, \theta_i}" style="vertical-align: 0px"/>.
In the single-objective case this is the optimal function value.
In the multi-objective case this is the hypervolume indicator of an approximation of the Pareto front <a class="reference internal" href="#bro2016" id="id27">[BRO2016]</a>.
Based on this reference value and a set of target <em>precision</em> values, which are
independent of the instance <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/>, we define a target value</p>
<div class="math">
<p><img src="_images/math/56f8b5b9c0c8db9a9e4b77cb38ec63b55f7fab40.png" alt="I^{\rm target,\theta_i} = I^{\rm ref,\theta_i} + \Delta I \enspace"/></p>
</div><p>for each precision <img class="math" src="_images/math/0fcd4c35133b823b54c75a11d1817b9cfa50acb0.png" alt="\Delta I" style="vertical-align: 0px"/>, giving rise to the product set of all problems <img class="math" src="_images/math/8aaa28094337e3019cc83e6c9ef7193a6412258c.png" alt="p^3" style="vertical-align: -4px"/> and all <img class="math" src="_images/math/0fcd4c35133b823b54c75a11d1817b9cfa50acb0.png" alt="\Delta I" style="vertical-align: 0px"/>-values.</p>
</div>
<div class="section" id="runlength-based-target-values">
<h3>Runlength-based Target Values<a class="headerlink" href="#runlength-based-target-values" title="Permalink to this headline">¶</a></h3>
<p>Runlength-based target values are a novel way to define the target values based
on a reference data set. Like for <em>performance profiles</em> <a class="reference internal" href="#dol2002" id="id28">[DOL2002]</a>, the
resulting empirical distribution can be interpreted <em>relative to a reference
algorithm or a set of reference algorithms</em>.
Unlike for performance profiles, the resulting empirical distribution <em>is</em> a
data profile <a class="reference internal" href="#mor2009" id="id29">[MOR2009]</a> reflecting the true (opposed to relative) difficulty of the respective problems for the respective algorithm.</p>
<p>We assume to have given a reference data set with recorded runtimes to reach a
prescribed, usually large set of quality indicator target values <a class="footnote-reference" href="#id31" id="id30">[8]</a> as in the
fixed-target approach described above.
The reference data serve as a baseline upon which the runlength-based targets are  computed.
To simplify wordings we assume w.l.o.g. that a single reference <em>algorithm</em> has generated this data set.</p>
<p>Now we choose a set of increasing reference <em>budgets</em>. To each budget, starting with the smallest, we associate the easiest (largest) target for which (i) the average runtime (taken over all respective <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/> instances, <img class="math" src="_images/math/27971c0d7cdc5cc52c6b37398061cd67d1106a1a.png" alt="\mathrm{aRT}" style="vertical-align: 0px"/>, see below) of the reference algorithm <em>exceeds</em> the budget and (ii, optionally) that had not been chosen for a smaller budget before. If such target does not exist, we take the final (smallest) target.</p>
<p>Like this, an algorithm that reaches a target within the associated budget is better than the reference algorithm on this problem.</p>
<p>Runlength-based targets are used in <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> for the single-objective expensive optimization scenario.
The artificial best algorithm of BBOB-2009 (see below) is used as reference algorithm with either the five budgets of <img class="math" src="_images/math/66051b7da8e19b8a922c6bc299617378558b9449.png" alt="0.5n" style="vertical-align: 0px"/>, <img class="math" src="_images/math/82b87d9c89e47f46c4ba74c51445bb1997a79bb0.png" alt="1.2n" style="vertical-align: -1px"/>, <img class="math" src="_images/math/3ca2b621bf8fe842c44b9779afca64f098e854ae.png" alt="3n" style="vertical-align: 0px"/>, <img class="math" src="_images/math/8eaea7a9edb0e75094964b04a5a7e39c0da801f6.png" alt="10n" style="vertical-align: -1px"/>, and <img class="math" src="_images/math/9010ab41b36043e1ac4713c8f939192136f12b08.png" alt="50n" style="vertical-align: 0px"/> function evaluations, where <img class="math" src="_images/math/7bc0d11b5a52732a23025dcc422625c0a5dde474.png" alt="n" style="vertical-align: 0px"/> is the problem
dimension, or with 31 targets evenly space on the log scale between <img class="math" src="_images/math/66051b7da8e19b8a922c6bc299617378558b9449.png" alt="0.5n" style="vertical-align: 0px"/> and <img class="math" src="_images/math/9010ab41b36043e1ac4713c8f939192136f12b08.png" alt="50n" style="vertical-align: 0px"/> and without the optional constraint from (ii) above. In the latter case, the empirical distribution function of the runtimes of the reference algorithm shown in a <code class="docutils literal"><span class="pre">semilogx</span></code> plot approximately resembles a diagonal straight line between the above two reference budgets.</p>
<p>Runlength-based targets have the <strong>advantage</strong> to make the target value setting less
dependent on the expertise of a human designer, because only the reference
<em>budgets</em> have to be chosen a priori. Reference budgets, as runtimes, are
intuitively meaningful quantities, on which it is comparatively easy to decide
upon.
Runlength-based targets have the <strong>disadvantage</strong> to depend on the choice of a reference data set, that is, they depend on a set of reference algorithms.</p>
<table class="docutils footnote" frame="void" id="id31" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id30">[8]</a></td><td>By default, the ratio between two neighboring <img class="math" src="_images/math/0fcd4c35133b823b54c75a11d1817b9cfa50acb0.png" alt="\Delta I" style="vertical-align: 0px"/> target precision values
is <img class="math" src="_images/math/996ff18e35860a9daef7b04f6206537290e7cc0f.png" alt="10^{0.2}" style="vertical-align: -1px"/> and the largest <img class="math" src="_images/math/0fcd4c35133b823b54c75a11d1817b9cfa50acb0.png" alt="\Delta I" style="vertical-align: 0px"/> value is (dynamically) chosen such
that the first evaluation of the worst algorithm hits the target.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="runtime-computation">
<h2>Runtime Computation<a class="headerlink" href="#runtime-computation" title="Permalink to this headline">¶</a></h2>
<p>In the performance assessment context of <a class="reference external" href="https://github.com/numbbo/coco">COCO</a>, a problem instance can be defined by the quintuple search space dimension, function, instantiation parameters, quality indicator mapping, and quality indicator target value, <img class="math" src="_images/math/c6be862e757aa661ff631a1c2ba6505f6468a6fc.png" alt="p^5 = p(n, f_\theta, \theta_i, I, I^{{\rm target}, \theta_i})" style="vertical-align: -4px"/>. <a class="footnote-reference" href="#id35" id="id32">[9]</a>
For each benchmarked algorithm, a single runtime is measured on each problem instance.
From a <em>single</em> run of the algorithm on the problem instance triple
<img class="math" src="_images/math/ab269c3f1c657b7a51c29aa7d5d998d048205e79.png" alt="p^3 = p(n, f_\theta, \theta_i)" style="vertical-align: -4px"/>, we obtain a runtime measurement for <em>each</em> corresponding problem quintuple <img class="math" src="_images/math/beae7dab2278797db0c5b826c63e5bc2cbca9cbc.png" alt="p^5" style="vertical-align: -4px"/>, more specifically, one for each target value which has been reached in this run, or equivalently, for each target precision.
This also reflects the anytime aspect of the performance evaluation in a single run.</p>
<p>Formally, the runtime <img class="math" src="_images/math/afe1d4d03f7dee03c3c52b6ceceee12c05f79858.png" alt="\mathrm{RT}^{\rm s}(p)" style="vertical-align: -4px"/> is a random variable that represents the number of function evaluations needed to reach the quality indicator target value for the first time.
A run or trial that reached the target value is called <em>successful</em>. <a class="footnote-reference" href="#id36" id="id33">[10]</a>
For <em>unsuccessful trials</em>, the runtime is not defined, but the overall number of function evaluations in the given trial is a random variable denoted by <img class="math" src="_images/math/cb5cac07aacc181a446bc944672e49ebb27e552e.png" alt="\mathrm{RT}^{\rm us}(p)" style="vertical-align: -4px"/>. For a single run, the value of <img class="math" src="_images/math/cb5cac07aacc181a446bc944672e49ebb27e552e.png" alt="\mathrm{RT}^{\rm us}(p)" style="vertical-align: -4px"/> is the same for all failed targets.</p>
<p>We consider the conceptual <strong>restart algorithm</strong>.
Given an algorithm has a strictly positive probability <img class="math" src="_images/math/c0edae9fa075d5a6fd4ec9b634612c282b37cbc5.png" alt="p_{\mathrm{s}}" style="vertical-align: -4px"/> to solve a
problem, independent restarts of the algorithm solve the problem with
probability one and exhibit the runtime</p>
<div class="math" id="equation-RTrestart">
<p><span class="eqno">(1)</span><img src="_images/math/ceff8ab5553dcdf74c9445fbed697d8152d44e1c.png" alt="\begin{equation*}%%remove*%%
\label{index-RTrestart}
  % &quot;:eq:`RTrestart`&quot; becomes &quot;\eqref{index-RTrestart}&quot; in the LaTeX
\mathbf{RT}(n, f_\theta, \Delta I) = \sum_{j=1}^{J} \mathrm{RT}^{\rm us}_j(n,f_\theta,\Delta I) + \mathrm{RT}^{\rm s}(n,f_\theta,\Delta I)
\enspace,
\end{equation*}%%remove*%%"/></p>
</div><p>where <img class="math" src="_images/math/3dd502bdc9645377aadeb307e0aca77ca0f91d28.png" alt="J \sim \mathrm{BN}(1, 1 - p_{\rm s})" style="vertical-align: -4px"/> is a random variable with negative binomial distribution that models the number of unsuccessful runs
until one success is observed and <img class="math" src="_images/math/7d2a56a8f78d7d64e6e5082aac6386b999ac15fd.png" alt="\mathrm{RT}^{\rm us}_j" style="vertical-align: -6px"/> are independent
random variables corresponding to the evaluations in unsuccessful trials
<a class="reference internal" href="#aug2005" id="id34">[AUG2005]</a>.
If the probability of success is one, <img class="math" src="_images/math/d45de93f61b7f7e0d212830cd2516e606339fc87.png" alt="J" style="vertical-align: 0px"/> equals zero with probability one and the restart algorithm coincides with the original algorithm.</p>
<p>Generally, the above equation for <img class="math" src="_images/math/a6e052b281750b1c25ddacb572a23a5c93647026.png" alt="\mathbf{RT}(n,f_\theta,\Delta I)" style="vertical-align: -4px"/> expresses the runtime from repeated independent runs on the same problem instance (while the instance <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/> is not given explicitly). For the performance evaluation in the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework, we apply the equation to runs on different instances <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/>, however instances from the same function, with the same dimension and the same target precision.</p>
<table class="docutils footnote" frame="void" id="id35" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id32">[9]</a></td><td>From the definition of <img class="math" src="_images/math/a5258a5b54a78f0ee95c7a5f85c29d72d2a13bcb.png" alt="p" style="vertical-align: -4px"/>, we can generate a set of problems <img class="math" src="_images/math/fbc0b5918e84cfff0c1f3683671c781b7cb47ac5.png" alt="\mathcal{P}" style="vertical-align: -1px"/> by varying one or several of the parameters. We never vary dimension <img class="math" src="_images/math/7bc0d11b5a52732a23025dcc422625c0a5dde474.png" alt="n" style="vertical-align: 0px"/> and always vary over all available instances <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/> for generating <img class="math" src="_images/math/1e4b6112920550e405482c11c04c9caf62f3dc16.png" alt="\mathcal{P}." style="vertical-align: -1px"/></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id36" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id33">[10]</a></td><td>The notion of success is directly linked to a target value. A run can be successful with respect to some target values (some problems) and unsuccessful with respect to others. Success also often refers to the final, most difficult, smallest target value, which implies success for all other targets.</td></tr>
</tbody>
</table>
<div class="section" id="runs-on-different-instances">
<h3>Runs on Different Instances<a class="headerlink" href="#runs-on-different-instances" title="Permalink to this headline">¶</a></h3>
<p>Different instantiations of the parametrized functions <img class="math" src="_images/math/ca2de076682c37daa8d0fb48ee82481772f5bb12.png" alt="f_{\theta}" style="vertical-align: -4px"/> are a natural way to represent randomized repetitions.
For example, different instances implement random translations of the search space and hence a translation of the optimum <a class="reference internal" href="#han2009fun" id="id37">[HAN2009fun]</a>.
Randomized restarts on the other hand can be conducted from different initial points.
For translation invariant algorithms both mechanisms are equivalent and can be mutually exchanged.</p>
<p>We interpret thus runs performed on different instances <img class="math" src="_images/math/f7ea92d2e8d3f99d2bcb7bdc018223b56aed11a3.png" alt="\theta_1, \ldots, \theta_K" style="vertical-align: -4px"/> as repetitions of the same problem.
Thereby we assume that instances of the same parametrized function <img class="math" src="_images/math/ca2de076682c37daa8d0fb48ee82481772f5bb12.png" alt="f_{\theta}" style="vertical-align: -4px"/> are
similar to each other, and more specifically that they exhibit the same runtime
distribution for each given <img class="math" src="_images/math/0fcd4c35133b823b54c75a11d1817b9cfa50acb0.png" alt="\Delta I" style="vertical-align: 0px"/>.</p>
<p>We hence have for each parametrized problem a set of <img class="math" src="_images/math/95652594d1af71370f41a93b3b7b0777eb6d2c30.png" alt="K\approx15" style="vertical-align: -1px"/> independent runs, which are used to compute artificial runtimes of the conceptual restart algorithm.</p>
</div>
<div class="section" id="simulated-restarts-and-runtimes">
<h3>Simulated Restarts and Runtimes<a class="headerlink" href="#simulated-restarts-and-runtimes" title="Permalink to this headline">¶</a></h3>
<p>The runtime of the conceptual restart algorithm as given in <a href="#equation-RTrestart">(1)</a> is the basis for displaying performance within <a class="reference external" href="https://github.com/numbbo/coco">COCO</a>.
We use the <img class="math" src="_images/math/6871010be8e4e18ffcfb86d3e29f745913c40e3f.png" alt="K" style="vertical-align: 0px"/> different runs on the same function and dimension to simulate virtual restarts with a fixed target precision.
We assume to have at least one successful run&#8212;otherwise, the runtime remains undefined, because the virtual procedure would never stop.
Then, we construct artificial, simulated runs from the available empirical data:
we repeatedly pick, uniformly at random with replacement, one of the <img class="math" src="_images/math/6871010be8e4e18ffcfb86d3e29f745913c40e3f.png" alt="K" style="vertical-align: 0px"/> trials until we encounter a successful trial.
This procedure simulates a single sample of the virtually restarted algorithm from the given data.
As given in <a href="#equation-RTrestart">(1)</a> as <img class="math" src="_images/math/a6e052b281750b1c25ddacb572a23a5c93647026.png" alt="\mathbf{RT}(n,f_\theta,\Delta I)" style="vertical-align: -4px"/>, the measured, simulated runtime is the sum of the number of function evaluations from the unsuccessful trials added to the runtime of the last and successful trial. <a class="footnote-reference" href="#id39" id="id38">[11]</a></p>
<table class="docutils footnote" frame="void" id="id39" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id38">[11]</a></td><td>In other words, we apply <a href="#equation-RTrestart">(1)</a> such that <img class="math" src="_images/math/f0b46606003b9e957f0799904e2f4d1a48b71435.png" alt="\mathrm{RT}^{\mathrm{s}}" style="vertical-align: 0px"/> is uniformly distributed over all measured runtimes from successful instances <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/>, <img class="math" src="_images/math/59e087290c77548b01e4ddfcb3df1c8a6e40bab1.png" alt="\mathrm{RT}^{\mathrm{us}}" style="vertical-align: 0px"/> is uniformly distributed over all evaluations seen in unsuccessful instances <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/>, and <img class="math" src="_images/math/d45de93f61b7f7e0d212830cd2516e606339fc87.png" alt="J" style="vertical-align: 0px"/> has a negative binomial distribution <img class="math" src="_images/math/4d3e7a00c56661776ddf4007b062c39cfec27539.png" alt="\mathrm{BN}(1, q)" style="vertical-align: -4px"/>, where <img class="math" src="_images/math/175cd047d8d944a033a834532b16c863c5cc3a70.png" alt="q" style="vertical-align: -4px"/> is the number of unsuccessful instance divided by all instances.</td></tr>
</tbody>
</table>
<div class="section" id="bootstrapping-runtimes">
<h4>Bootstrapping Runtimes<a class="headerlink" href="#bootstrapping-runtimes" title="Permalink to this headline">¶</a></h4>
<p>In practice, we repeat the above procedure between a hundred or even thousand times, thereby sampling <img class="math" src="_images/math/a6f76494d872a906ba21e4ce84b3c10d9fa6cd8d.png" alt="N" style="vertical-align: 0px"/> simulated runtimes from the same underlying distribution,
which then has striking similarities with the true distribution from a restarted algorithm <a class="reference internal" href="#efr1994" id="id40">[EFR1994]</a>.
To reduce the variance in this procedure, when desired, the first trial in each sample is picked deterministically instead of randomly as the <img class="math" src="_images/math/3788b04c7530655601249b910e6db4f1816eedd9.png" alt="1 + (N~\mathrm{mod}~K)" style="vertical-align: -4px"/>-th trial from the data. <a class="footnote-reference" href="#id42" id="id41">[12]</a>
Picking the first trial data as specific instance <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/> could also be
interpreted as applying simulated restarts to this specific instance rather than
to the entire set of problems <img class="math" src="_images/math/663fc0634d623f152a704e3d9f62b1e645eeeec7.png" alt="\mathcal{P} = \{p(n, f_\theta, \theta_i, \Delta I) \;|\;
i=1,\dots,K\}" style="vertical-align: -5px"/>.</p>
<table class="docutils footnote" frame="void" id="id42" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id41">[12]</a></td><td>The variance reducing effect is best exposed in the case where all runs are successful and <img class="math" src="_images/math/0fafc1a080cd4c4baf63f6083eb3eac6fcc8f10c.png" alt="N = K" style="vertical-align: 0px"/>, in which case each data is picked exactly once.
This example also suggests to apply a random permutation of the data before to simulate virtually restarted runs.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="rationales-and-limitations">
<h4>Rationales and Limitations<a class="headerlink" href="#rationales-and-limitations" title="Permalink to this headline">¶</a></h4>
<p>Simulated restarts aggregate some of the available data and thereby extend their range of interpretation.</p>
<ul class="simple">
<li>Simulated restarts allow in particular to compare algorithms with a wide range of different success probabilities by a single performance measure. <a class="footnote-reference" href="#id45" id="id43">[13]</a> Conducting restarts is also valuable approach when addressing a difficult optimization problem in practice.</li>
<li>Simulated restarts rely on the assumption that the runtime distribution for each instance is the same. If this is not the case, they still provide a reasonable performance measure, however with less of a meaningful interpretation for the result.</li>
<li>The runtime of simulated restarts may heavily depend on <strong>termination conditions</strong> applied in the benchmarked algorithm, due to the evaluations spent in unsuccessful trials, compare <a href="#equation-RTrestart">(1)</a>. This can be interpreted as disadvantage, when termination is considered as a trivial detail in the implementation&#8212;or as an advantage, when termination is considered a relevant component in the practical application of numerical optimization algorithms.</li>
<li>The maximal number of evaluations for which simulated runtimes are meaningful
and representative depends on the experimental conditions. If all runs are successful, no restarts are simulated and all runtimes are meaningful. If all runs terminated due to standard termination conditions in the used algorithm, simulated restarts reflect the original algorithm. However, if a maximal budget is imposed for the purpose of benchmarking, simulated restarts do not necessarily reflect the real performance. In this case and if the success probability drops below 1/2, the result is likely to give a too pessimistic viewpoint at or beyond the chosen maximal budget. See <a class="reference internal" href="#han2016ex" id="id44">[HAN2016ex]</a> for a more in depth discussion on how to setup restarts in the experiments.</li>
<li>If only few or no successes have been observed, we can see large effects without statistical significance. Namely, 4/15 successes are not statistically significant against 0/15 successes on a 5%-level.</li>
</ul>
<table class="docutils footnote" frame="void" id="id45" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id43">[13]</a></td><td>The range of success probabilities is bounded by the number of instances to roughly <img class="math" src="_images/math/d0c5bee4c5b1c17a8e63bf65cbe63f24de0966a5.png" alt="2/|K|." style="vertical-align: -5px"/></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="averaging-runtime">
<span id="sec-art"></span><h2>Averaging Runtime<a class="headerlink" href="#averaging-runtime" title="Permalink to this headline">¶</a></h2>
<p>The average runtime (<img class="math" src="_images/math/27971c0d7cdc5cc52c6b37398061cd67d1106a1a.png" alt="\mathrm{aRT}" style="vertical-align: 0px"/>), introduced in <a class="reference internal" href="#pri1997" id="id46">[PRI1997]</a> as ENES and
analyzed in <a class="reference internal" href="#aug2005" id="id47">[AUG2005]</a> as success performance and referred to as
ERT in <a class="reference internal" href="#han2009ex" id="id48">[HAN2009ex]</a>, estimates the expected runtime of the restart
algorithm given in <a href="#equation-RTrestart">(1)</a>. Generally, the set of trials
over which the average is taken is generated by varying <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/> only.</p>
<p>We compute the <img class="math" src="_images/math/27971c0d7cdc5cc52c6b37398061cd67d1106a1a.png" alt="\mathrm{aRT}" style="vertical-align: 0px"/> from a set of trials as the sum of all evaluations in unsuccessful trials plus the sum of the runtimes in all successful trials, both divided by the number of successful trials.</p>
<div class="section" id="motivation">
<h3>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h3>
<p>The expected runtime of the restart algorithm writes <a class="reference internal" href="#aug2005" id="id49">[AUG2005]</a></p>
<div class="math">
<p><img src="_images/math/41a53d9a72c746606b270e8a23edd49e9a297056.png" alt="\begin{eqnarray*}
\mathbb{E}(\mathbf{RT}) &amp; =
&amp; \mathbb{E}(\mathrm{RT}^{\rm s})  + \frac{1-p_\mathrm{s}}{p_\mathrm{s}}
  \mathbb{E}(\mathrm{RT}^{\rm us})
\enspace,
\end{eqnarray*}"/></p>
</div><p>where <img class="math" src="_images/math/1ff1da5bae792ec2e9e2de1e95293d89331b1f7a.png" alt="p_\mathrm{s} &gt; 0" style="vertical-align: -4px"/> is the probability of success of the algorithm and notations from above are used.</p>
<p>Given a data set with <img class="math" src="_images/math/b0ae62ed3cf403a71ad7e23e3a43026ed857c302.png" alt="n_\mathrm{s}\ge1" style="vertical-align: -3px"/> successful runs with runtimes <img class="math" src="_images/math/712bacc309bbebf3a4c71f3b22ee8671e2adafae.png" alt="\mathrm{RT}^{\rm s}_i" style="vertical-align: -4px"/>, and <img class="math" src="_images/math/ce7b9174f9097a8224640f37bb5809b72dcafdb0.png" alt="n_\mathrm{us}" style="vertical-align: -3px"/> unsuccessful runs with <img class="math" src="_images/math/7d2a56a8f78d7d64e6e5082aac6386b999ac15fd.png" alt="\mathrm{RT}^{\rm us}_j" style="vertical-align: -6px"/> evaluations, the average runtime reads</p>
<div class="math">
<p><img src="_images/math/71372d0b6655e9d5605d10d7d96da3624a4200f4.png" alt="\begin{eqnarray*}
\mathrm{aRT}
&amp; = &amp;
\frac{1}{n_\mathrm{s}} \sum_i \mathrm{RT}^{\rm s}_i +
\frac{1-p_{\mathrm{s}}}{p_{\mathrm{s}}}\,
\frac{1}{n_\mathrm{us}} \sum_j \mathrm{RT}^{\rm us}_j
\\
&amp; = &amp;
\frac{\sum_i \mathrm{RT}^{\rm s}_i + \sum_j \mathrm{RT}^{\rm us}_j }{n_\mathrm{s}}
\\
&amp; = &amp;
\frac{\#\mathrm{FEs}}{n_\mathrm{s}}
\end{eqnarray*}"/></p>
</div><p>where <img class="math" src="_images/math/c0edae9fa075d5a6fd4ec9b634612c282b37cbc5.png" alt="p_{\mathrm{s}}" style="vertical-align: -4px"/> is the fraction of successful trials, <img class="math" src="_images/math/bd30ef91186731f138a4efbe0b733d16938fc5d6.png" alt="0/0" style="vertical-align: -5px"/> is
understood as zero and <img class="math" src="_images/math/c3107cbbdacc67f032c2143b252962150f161f64.png" alt="\#\mathrm{FEs}" style="vertical-align: -3px"/> is the number of function
evaluations conducted in all trials before to reach the given target precision.</p>
</div>
<div class="section" id="rationale-and-limitations">
<h3>Rationale and Limitations<a class="headerlink" href="#rationale-and-limitations" title="Permalink to this headline">¶</a></h3>
<p>The average runtime, <img class="math" src="_images/math/27971c0d7cdc5cc52c6b37398061cd67d1106a1a.png" alt="\mathrm{aRT}" style="vertical-align: 0px"/>, is taken over different instances of the same function, dimension, and target precision, as these instances are interpreted as repetitions.
Taking the average is meaningful only if each instance obeys a similar distribution without heavy tail.
If one instance is considerably harder than the others, the average is dominated by this instance.
For this reason we do not average runtimes from different functions or different target precisions, which however could be done if the logarithm is taken first (geometric average).
Plotting the <img class="math" src="_images/math/27971c0d7cdc5cc52c6b37398061cd67d1106a1a.png" alt="\mathrm{aRT}" style="vertical-align: 0px"/> divided by dimension against dimension in a log-log plot is the recommended way to investigate the scaling behavior of an algorithm.</p>
</div>
</div>
<div class="section" id="empirical-distribution-functions">
<span id="sec-ecdf"></span><h2>Empirical Distribution Functions<a class="headerlink" href="#empirical-distribution-functions" title="Permalink to this headline">¶</a></h2>
<p>We display a set of simulated runtimes with the empirical cumulative
distribution function (ECDF), AKA empirical distribution function.
Informally, the ECDF displays the <em>proportion of problems solved within a
specified budget</em>, where the budget is given on the <img class="math" src="_images/math/08c8ce72dec4fe55a32d5a0c9cfb153e20cc7329.png" alt="x" style="vertical-align: 0px"/>-axis.
More formally, an ECDF gives for each <img class="math" src="_images/math/08c8ce72dec4fe55a32d5a0c9cfb153e20cc7329.png" alt="x" style="vertical-align: 0px"/>-value the fraction of runtimes which do not exceed <img class="math" src="_images/math/08c8ce72dec4fe55a32d5a0c9cfb153e20cc7329.png" alt="x" style="vertical-align: 0px"/>, where missing runtime values are counted in the denominator of the fraction.</p>
<div class="section" id="rationale-interpretation-and-limitations">
<h3>Rationale, Interpretation and Limitations<a class="headerlink" href="#rationale-interpretation-and-limitations" title="Permalink to this headline">¶</a></h3>
<p>Empirical cumulative distribution functions are a universal way to display <em>unlabeled</em> data in a condensed way without losing information.
They allow unconstrained aggregation, because each data point remains separately displayed, and they remain entirely meaningful under transformation of the data (e.g. taking the logarithm).</p>
<ul>
<li><p class="first">The empirical distribution function from a set of problems where only the target value varies, recovers an upside-down convergence graph with the resolution steps defined by the targets <a class="reference internal" href="#han2010" id="id50">[HAN2010]</a>.</p>
</li>
<li><p class="first">When runs from several instances are aggregated, the association to the single run is lost, as is the association to the function when aggregating over several functions. This is particularly problematic for data from different dimensions, because dimension can be used as decision parameter for algorithm selection. Therefore, we do not aggregate over dimension.</p>
</li>
<li><p class="first">The empirical distribution function can be read in two distinct ways.</p>
<dl class="docutils">
<dt><img class="math" src="_images/math/08c8ce72dec4fe55a32d5a0c9cfb153e20cc7329.png" alt="x" style="vertical-align: 0px"/>-axis as independent variable:</dt>
<dd><p class="first last">for any budget (<img class="math" src="_images/math/08c8ce72dec4fe55a32d5a0c9cfb153e20cc7329.png" alt="x" style="vertical-align: 0px"/>-value),
we see the fraction of problems solved within the budget as <img class="math" src="_images/math/c9aa8fdd7a1793b0e8b990f68ade98c3b3b67ea3.png" alt="y" style="vertical-align: -4px"/>-value, where
the limit value to the right is the fraction of solved problems with the maximal
budget.</p>
</dd>
<dt><img class="math" src="_images/math/c9aa8fdd7a1793b0e8b990f68ade98c3b3b67ea3.png" alt="y" style="vertical-align: -4px"/>-axis as independent variable:</dt>
<dd><p class="first last">for any fraction of easiest problems
(<img class="math" src="_images/math/c9aa8fdd7a1793b0e8b990f68ade98c3b3b67ea3.png" alt="y" style="vertical-align: -4px"/>-value), we see the maximal runtime observed on these problems on the
<img class="math" src="_images/math/08c8ce72dec4fe55a32d5a0c9cfb153e20cc7329.png" alt="x" style="vertical-align: 0px"/>-axis. When plotted in <code class="docutils literal"><span class="pre">semilogx</span></code>, a horizontal shift indicates a runtime
difference by the respective factor, quantifiable, e.g., as &#8220;five times
faster&#8221;. The area below the <img class="math" src="_images/math/c9aa8fdd7a1793b0e8b990f68ade98c3b3b67ea3.png" alt="y" style="vertical-align: -4px"/>-value and to the left of the graph reflects
the geometric runtime average on this subset of problems, the smaller the
better.</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="relation-to-previous-work">
<h3>Relation to Previous Work<a class="headerlink" href="#relation-to-previous-work" title="Permalink to this headline">¶</a></h3>
<p>Empirical distribution functions over runtimes of optimization algorithms are also known as <em>data profiles</em> <a class="reference internal" href="#mor2009" id="id51">[MOR2009]</a>.
They are widely used for aggregating results from different functions and different dimensions to reach a single target precision <a class="reference internal" href="#rio2012" id="id52">[RIO2012]</a>.
In the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> framework, we do not aggregation over dimension but aggregate often over a wide range of target precision values.</p>
</div>
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>We display in Figure <a class="reference internal" href="#fig-ecdf"><span>ECDF</span></a> the ECDF of the (simulated) runtimes of
the pure random search algorithm on the set of problems formed by 15 instances of the sphere function (first function of the single-objective <code class="docutils literal"><span class="pre">bbob</span></code> test
suite) in dimension <img class="math" src="_images/math/9e53fe387cd561314d4b70a956eacff1a18cfea1.png" alt="n=5" style="vertical-align: 0px"/> each with 51 target precisions between <img class="math" src="_images/math/bcb76298b44d7bf1063a0bb836ba31ee8208c28b.png" alt="10^2" style="vertical-align: -1px"/> and <img class="math" src="_images/math/2c3fcbf921863c16bc29bdc9481670e698c60981.png" alt="10^{-8}" style="vertical-align: -1px"/> uniform on a log-scale and 1000 bootstraps.</p>
<div class="figure align-center" id="id65">
<span id="fig-ecdf"></span><a class="reference internal image-reference" href="_images/pprldmany_f001_05D.png"><img alt="_images/pprldmany_f001_05D.png" src="_images/pprldmany_f001_05D.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-text">ECDF</span></p>
<div class="legend">
Illustration of empirical (cumulative) distribution function (ECDF) of
runtimes on the sphere function using 51 relative targets uniform on a log
scale between <img class="math" src="_images/math/bcb76298b44d7bf1063a0bb836ba31ee8208c28b.png" alt="10^2" style="vertical-align: -1px"/> and <img class="math" src="_images/math/2c3fcbf921863c16bc29bdc9481670e698c60981.png" alt="10^{-8}" style="vertical-align: -1px"/>. The runtimes displayed
correspond to the pure random search algorithm in dimension 5. The cross on
the ECDF plots of <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> represents the median of the maximal length of the
unsuccessful runs to solve the problems aggregated within the ECDF.</div>
</div>
<p>We can see in this plot, for example, that almost 20 percent of the problems
were solved within <img class="math" src="_images/math/808768eefff744db0d42386a24ba55d8f7315243.png" alt="10^3 \cdot n = 5 \cdot 10^3" style="vertical-align: -1px"/> function evaluations.
Runtimes to the right of the cross at <img class="math" src="_images/math/6febd02160f63eb453836a90407ca6b1bc2fa755.png" alt="10^6" style="vertical-align: -1px"/> have at least one unsuccessful run.
This can be concluded, because with pure random search each unsuccessful run exploits the maximum budget.
The small dot beyond <img class="math" src="_images/math/ad6b31b3feb3f35911615358b4858f05bf50b2b7.png" alt="x=10^7" style="vertical-align: -1px"/> depicts the overall fraction of all successfully solved functions-target pairs, i.e., the fraction of <img class="math" src="_images/math/e84e1c433c5125da1decc3bedf3119c96fe3bdc2.png" alt="(f_\theta, \Delta I)" style="vertical-align: -4px"/> pairs for which at least one trial (one <img class="math" src="_images/math/7b17158cd34922d55a6b455d473d4b16fa5014b4.png" alt="\theta_i" style="vertical-align: -3px"/> instantiation) was successful.</p>
<p>We usually divide the set of all (parametrized) benchmark
functions into subgroups sharing similar properties (for instance
separability, unimodality, ...) and display ECDFs which aggregate the
problems induced by these functions and all targets.
Figure <a class="reference internal" href="#fig-ecdfgroup"><span>ECDF for a subgroup of functions</span></a> shows the result of random search on the first
five functions of the <cite>bbob</cite> testsuite, separate (left) and aggregated (right).</p>
<div class="figure align-center" id="id66">
<span id="fig-ecdfgroup"></span><a class="reference internal image-reference" href="_images/gr_separ_05D_05D_separ-combined.png"><img alt="_images/gr_separ_05D_05D_separ-combined.png" src="_images/gr_separ_05D_05D_separ-combined.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">ECDF for a subgroup of functions</span></p>
<div class="legend">
<strong>Left:</strong> ECDF of the runtime of the pure random search algorithm for
functions f1, f2, f3, f4 and f5 that constitute the group of
separable functions for the <code class="docutils literal"><span class="pre">bbob</span></code> testsuite over 51 target values.
<strong>Right:</strong> Aggregated ECDF of the same data, that is, all functions
in one graph.</div>
</div>
<p>Finally, we also naturally aggregate over all functions of the benchmark and
hence obtain one single ECDF per algorithm per dimension.
In Figure <a class="reference internal" href="#fig-ecdfall"><span>ECDF over all functions and all targets</span></a>, the ECDF of different algorithms are displayed in
a single plot.</p>
<div class="figure align-center" id="id67">
<span id="fig-ecdfall"></span><a class="reference internal image-reference" href="_images/pprldmany_noiselessall-5and20D.png"><img alt="_images/pprldmany_noiselessall-5and20D.png" src="_images/pprldmany_noiselessall-5and20D.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">ECDF over all functions and all targets</span></p>
<div class="legend">
ECDF of several algorithms benchmarked during the BBOB 2009 workshop
in dimension 5 (left) and in dimension 20 (right) when aggregating over all functions of the <code class="docutils literal"><span class="pre">bbob</span></code> suite.</div>
</div>
<p>The thick maroon line with diamond markers annotated as &#8220;best 2009&#8221; corresponds to the <strong>artificial best 2009 algorithm</strong>: for
each set of problems with the same function, dimension and target precision, we select the algorithm with the smallest <img class="math" src="_images/math/27971c0d7cdc5cc52c6b37398061cd67d1106a1a.png" alt="\mathrm{aRT}" style="vertical-align: 0px"/> from the <a class="reference external" href="https://numbbo.github.io/coco/workshops">BBOB-2009 workshop</a> and use for these problems the data from the selected algorithm.
The algorithm is artificial because we may use even for different target values the runtime results from different algorithms. <a class="footnote-reference" href="#id55" id="id53">[14]</a></p>
<p>We observe that the artificial best 2009 algorithm is about two to three time faster than the left envelope of all single algorithms and solves all problems in about <img class="math" src="_images/math/540a2b9cc68bcc02770d07670bdc6bc24b36678e.png" alt="10^7\, n" style="vertical-align: -1px"/> function evaluations.</p>
<table class="docutils footnote" frame="void" id="id55" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id53">[14]</a></td><td>The best 2009 curve is not guaranteed to be an upper
left envelope of the ECDF of all algorithms from which it is
constructed, that is, the ECDF of an algorithm from BBOB-2009 can
cross the best 2009 curve. This may typically happen if an algorithm
has for the most easy problems a large runtime variation its <img class="math" src="_images/math/27971c0d7cdc5cc52c6b37398061cd67d1106a1a.png" alt="\mathrm{aRT}" style="vertical-align: 0px"/> is
not the best but the short runtimes
show up to the left of the best 2009 graph.</td></tr>
</tbody>
</table>
<H2>Acknowledgments</H2><p>This work was supported by the grant ANR-12-MONU-0009 (NumBBO)
of the French National Research Agency.</p>
<p>The authors would like to thank Raymond Ros, Steffen Finck, Marc Schoenauer and
Petr Posik for their many invaluable contributions to this work.</p>
<H2>References</H2><table class="docutils citation" frame="void" id="aug2005" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[AUG2005]</td><td><em>(<a class="fn-backref" href="#id34">1</a>, <a class="fn-backref" href="#id47">2</a>, <a class="fn-backref" href="#id49">3</a>)</em> A. Auger and N. Hansen (2005). Performance evaluation of an advanced
local search evolutionary algorithm. In <em>Proceedings of the IEEE Congress on
Evolutionary Computation (CEC 2005)</em>, pages 1777–1784.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="aug2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id20">[AUG2009]</a></td><td>A. Auger, N. Hansen, J.M. Perez Zerpa, R. Ros and M. Schoenauer (2009).
Empirical comparisons of several derivative free optimization algorithms. In Acte du 9ime colloque national en calcul des structures, Giens.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="bro2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[BRO2016]</td><td><em>(<a class="fn-backref" href="#id18">1</a>, <a class="fn-backref" href="#id27">2</a>)</em> D. Brockhoff, T. Tušar, D. Tušar, T. Wagner, N. Hansen,
A. Auger (2016). <a class="reference external" href="http://numbbo.github.io/coco-doc/bbob-biobj/perf-assessment">Biobjective Performance Assessment with the COCO Platform</a>. <em>ArXiv e-prints</em>, <a class="reference external" href="http://arxiv.org/abs/1605.01746">arXiv:1605.01746</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="dol2002" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id28">[DOL2002]</a></td><td>E.D. Dolan, J.J. Moré (2002). Benchmarking optimization software
with performance profiles. <em>Mathematical Programming</em> 91.2, 201-213.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="efr1994" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id40">[EFR1994]</a></td><td>B. Efron and R. Tibshirani (1994). <em>An introduction to the
bootstrap</em>. CRC Press.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2009ex" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id48">[HAN2009ex]</a></td><td>N. Hansen, A. Auger, S. Finck, and R. Ros (2009). Real-Parameter
Black-Box Optimization Benchmarking 2009: Experimental Setup,
<a class="reference external" href="http://hal.inria.fr/inria-00362649/en">Research Report RR-6828</a>, Inria.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2016co" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[HAN2016co]</a></td><td>N. Hansen, A. Auger, O. Mersmann, T. Tušar, D. Brockhoff (2016).
<a class="reference external" href="http://numbbo.github.io/coco-doc/">COCO: A Platform for Comparing Continuous Optimizers in a Black-Box
Setting</a>. <em>ArXiv e-prints</em>, <a class="reference external" href="http://arxiv.org/abs/1603.08785">arXiv:1603:08785</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2010" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id50">[HAN2010]</a></td><td>N. Hansen, A. Auger, R. Ros, S. Finck, and P. Posik (2010).
Comparing Results of 31 Algorithms from the Black-Box Optimization
Benchmarking BBOB-2009. Workshop Proceedings of the GECCO Genetic and
Evolutionary Computation Conference 2010, ACM, pp. 1689-1696</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2009fun" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[HAN2009fun]</td><td><em>(<a class="fn-backref" href="#id9">1</a>, <a class="fn-backref" href="#id37">2</a>)</em> N. Hansen, S. Finck, R. Ros, and A. Auger (2009).
Real-parameter black-box optimization benchmarking 2009: Noiseless
functions definitions. <a class="reference external" href="https://hal.inria.fr/inria-00362633">Research Report RR-6829</a>, Inria, updated
February 2010.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2016ex" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[HAN2016ex]</td><td><em>(<a class="fn-backref" href="#id8">1</a>, <a class="fn-backref" href="#id16">2</a>, <a class="fn-backref" href="#id44">3</a>)</em> N. Hansen, T. Tušar, A. Auger, D. Brockhoff, O. Mersmann (2016).
<a class="reference external" href="http://numbbo.github.io/coco-doc/experimental-setup/">COCO: The Experimental Procedure</a>, <em>ArXiv e-prints</em>, <a class="reference external" href="http://arxiv.org/abs/1603.08776">arXiv:1603.08776</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="hoo1995" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[HOO1995]</a></td><td>J.N. Hooker (1995). Testing heuristics: We have it all wrong. In Journal of
Heuristics, pages 33-42.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="hoo1998" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[HOO1998]</td><td>H.H. Hoos and T. Stützle. Evaluating Las Vegas
algorithms—pitfalls and remedies. In <em>Proceedings of the Fourteenth
Conference on Uncertainty in Artificial Intelligence (UAI-98)</em>,
pages 238–245, 1998.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mor2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[MOR2009]</td><td><em>(<a class="fn-backref" href="#id29">1</a>, <a class="fn-backref" href="#id51">2</a>)</em> J.J. Moré and S.M. Wild (2009). Benchmarking
Derivative-Free Optimization Algorithms, <em>SIAM J. Optim.</em>, 20(1), 172–191.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pri1997" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id46">[PRI1997]</a></td><td>K. Price (1997). Differential evolution vs. the functions of
the second ICEO. In Proceedings of the IEEE International Congress on
Evolutionary Computation, pages 153–157.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="rio2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id52">[RIO2012]</a></td><td>L.M. Rios and N.V. Sahinidis (2013). Derivative-free optimization:
A review of algorithms and comparison of software implementations.
Journal of Global Optimization, 56(3):1247– 1293.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ste1946" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id10">[STE1946]</a></td><td>S.S. Stevens (1946).
On the theory of scales of measurement. <em>Science</em> 103(2684), pp. 677-680.</td></tr>
</tbody>
</table>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="nav-item nav-item-0"><a href="#">COCO: Performance Assessment</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
      Last updated on May 13, 2016.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.
    </div>
  </body>
</html>